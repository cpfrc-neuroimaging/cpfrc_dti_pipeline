<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html lang="en" class="dark">
<!--<![endif]-->

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="keywords" content="">

    <title>CPFRC Diffusion Pipeline Documentation | University of Michigan</title>

    <link rel="shortcut icon" href="images/UM.ico" type="image/x-icon">

    <link rel="stylesheet" type="text/css" href="fonts/font-awesome-4.3.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="css/stroke.css">
    <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
    <link rel="stylesheet" type="text/css" href="css/animate.css">
    <link rel="stylesheet" type="text/css" href="css/prettyPhoto.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">

    <link rel="stylesheet" type="text/css" href="js/syntax-highlighter/styles/shCore.css" media="all">
    <link rel="stylesheet" type="text/css" href="js/syntax-highlighter/styles/shThemeRDark.css" media="all">

    <!-- CUSTOM -->
    <link rel="stylesheet" type="text/css" href="css/custom.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>
    <button onclick="topFunction()" id="myBtn" title="Go to top"><i class="fa fa-chevron-up" aria-hidden="true"></i></button>

    <script>
        var mybutton = document.getElementById("myBtn");
        window.onscroll = function() {scrollFunction()};
        function scrollFunction() {
            if (document.body.scrollTop > 1000 || document.documentElement.scrollTop > 1000) {
                mybutton.style.display = "block";
            } else {
                mybutton.style.display = "none";
            }
        }
        function topFunction() {
            window.scrollTo({ top: 0, behavior: 'smooth' })
            document.documentElement.scrollTo({ top: 0, behavior: 'smooth' })
        }

        document.addEventListener("DOMContentLoaded", () => {
            document.querySelector('#mode').addEventListener('click',()=>{
                document.querySelector('html').classList.toggle('dark');
            })
        });


    </script>

    <div id="wrapper">

        <div id="mode" >
            <div class="dark">
                <svg aria-hidden="true" viewBox="0 0 512 512">
                    <title>lightmode</title>
                    <path fill="currentColor" d="M256 160c-52.9 0-96 43.1-96 96s43.1 96 96 96 96-43.1 96-96-43.1-96-96-96zm246.4 80.5l-94.7-47.3 33.5-100.4c4.5-13.6-8.4-26.5-21.9-21.9l-100.4 33.5-47.4-94.8c-6.4-12.8-24.6-12.8-31 0l-47.3 94.7L92.7 70.8c-13.6-4.5-26.5 8.4-21.9 21.9l33.5 100.4-94.7 47.4c-12.8 6.4-12.8 24.6 0 31l94.7 47.3-33.5 100.5c-4.5 13.6 8.4 26.5 21.9 21.9l100.4-33.5 47.3 94.7c6.4 12.8 24.6 12.8 31 0l47.3-94.7 100.4 33.5c13.6 4.5 26.5-8.4 21.9-21.9l-33.5-100.4 94.7-47.3c13-6.5 13-24.7.2-31.1zm-155.9 106c-49.9 49.9-131.1 49.9-181 0-49.9-49.9-49.9-131.1 0-181 49.9-49.9 131.1-49.9 181 0 49.9 49.9 49.9 131.1 0 181z"></path>
                </svg>
            </div>
            <div class="light">
                <svg aria-hidden="true" viewBox="0 0 512 512">
                    <title>darkmode</title>
                    <path fill="currentColor" d="M283.211 512c78.962 0 151.079-35.925 198.857-94.792 7.068-8.708-.639-21.43-11.562-19.35-124.203 23.654-238.262-71.576-238.262-196.954 0-72.222 38.662-138.635 101.498-174.394 9.686-5.512 7.25-20.197-3.756-22.23A258.156 258.156 0 0 0 283.211 0c-141.309 0-256 114.511-256 256 0 141.309 114.511 256 256 256z"></path>
                </svg>
            </div>
        </div>

        <div class="container">

            <section id="top" class="section docs-heading">

                <div class="row">
                    <div class="col-md-12">
                        <div class="big-title text-center">
                            <h1>CPFRC Diffusion Pipeline | University of Michigan</h1>
                            <p class="lead">Documentation Version 1.0</p>
                        </div>
                        <!-- end title -->
                    </div>
                    <!-- end 12 -->
                </div>
                <!-- end row -->

                <hr>

            </section>
            <!-- end section -->

            <div class="row">

                <div class="col-md-3">
                    <nav class="docs-sidebar" data-spy="affix" data-offset-top="300" data-offset-bottom="200" role="navigation">
                        <ul class="nav">
                            <li><a href="#line1">Mrtrix and Data Organization</a></li>
                            <li><a href="#line2">Creating Fieldmaps</a>
                                <ul class="nav">
                                    <li><a href="#line2_1">Create Fieldmap Script</a></li>
                                    <li><a href="#line2_2">fslFMAP Script</a></li>
                                </ul>
                            </li>
                            <li><a href="#line3">Copy and Convert the Data</a></li>
                            <li><a href="#line4">DWI Denoise</a></li>
                            <li><a href="#line5">Working on Armis</a></li>
                            <li><a href="#line6">dwifslpreproc</a>
                                <ul class="nav">
                                    <li><a href="#line6_1">Preproc Sbatch Script</a></li>
                                    <li><a href="#line6_2">Preproc Job Script</a></li>
                                </ul>
                            </li>
                            <li><a href="#line7">Spherical Deconvolution</a></li>
                            <li><a href="#line8">Create Tissue Boundaries</a></li>
                            <li><a href="#line9">FreeSurfer Recon-all</a>
                                <ul class="nav">
                                    <li><a href="#line9_1">Recon-all Sbatch Script</a></li>
                                    <li><a href="#line9_2">Recon-all Job Script</a></li>
                                </ul>
                            </li>
                            <li><a href="#line10">Generating Streamlines</a>
                                <ul class="nav">
                                    <li><a href="#line10_1">Streamlines Sbatch Script</a></li>
                                    <li><a href="#line10_2">Streamlines Job Script</a></li>
                                </ul>
                            </li>
                            <li><a href="#line11">Creating the Connectome</a></li>
                        </ul>
                    </nav >
                </div>
                <div class="col-md-9">
                    <section class="welcome">

                        <div class="row">
                            <div class="col-md-12 left-align">
                               <h2 class="dark-text">Introduction<hr></h2>
                                <div class="row">

                                    <div class="col-md-12 full">
                                        <div class="intro1">
                                            <ul>
                                                <li><strong>Project Name : </strong>Diffusion Pipeline Docs</li>
                                                <li><strong>Project Version : </strong> v1.0</li>
                                                <li><strong>Author  : </strong>Daniel Asay</li>
                                            </ul>
                                        </div>

                                        <hr>
                                        <div>
                                            <p>This documentation aims to serve as a guide for using the CPFRC Diffusion pipeline. The steps include everything from starting with the raw data from the fMRI Lab to creating a connectome for invidual subjects. Our pipeline draws heavily from the tutorial by Andy Jahn which can be found <a href="https://andysbrainbook.readthedocs.io/en/latest/MRtrix/MRtrix_Introduction.html" target="_blank">here</a>. It has been modified for our computing environment and largely transposed into Python. That being said, most of what's found here is broadly applicable beyond the CPFRC computing environment. 
                                            </p>

                                            <h4>Software Requirements</h4>
                                            <p>You will need the following sofware packages to follow this pipeline:</p>
                                            <ol>
                                                <li>Python version >= 3.8 </li>
                                                <li>MRtrix version 3.0.3 (install <a href="https://mrtrix.readthedocs.io/en/latest/installation/before_install.html" target="_blank">instructions</a>)</li>
                                                <li>FSL version >= 6.0.0 (install <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation" target="_blank">instructions</a>)</li>
                                                <li>FreeSurfer version >= 6.0 (install <a href="https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall" target="_blank">instructions</a>)</li>
                                                <li>ANTs (install <a href="http://stnava.github.io/ANTs/" target="_blank">instructions</a>)</li>
                                            </ol>
                                        </div>
                                    </div>

                                </div>
                                <!-- end row -->
                            </div>
                        </div>
                    </section>

                    <section id="line1" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">MRtrix and Data Organization<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">
                            <div class="col-md-12">

                                <h4>MRtrix</h4>

                                <p>MRtrix is a software package developed with the goal of improving the analysis of diffusion weighted images using constrained spherical deconvolution as opposed to tensor-fitting techniques. This technique aims to address the problem of crossing fibers within voxels that can be a confound when fitting a tensor. In short, it's a program that will suite our needs nicely. Please note that MRtrix makes use of external software, such as FSL. 
                                </p>

                                <p>To use MRtrix both on the server and Armis, we will use the same commands: <strong>module load mrtrix</strong> and <strong>module load fsl/6.0.3</strong>. If you're working on the server the <strong>module list</strong> command should list mrtrix/3.0_RC3 and fsl/6.0.3 as loaded modules. If you're working on Armis, the loading of the fsl and cuda modules is actually baked into the <strong>module load mrtrix</strong> command, so you should see three modules including: fsl/6.0.5.1, cuda/10.2.89 and mrtrix/3.0.3 after running module load mrtrix. If you do not see all of these modules loaded, please connact Bennet Fauber at HITS (bennet@umich.edu).

                                </p>

                                <h4>Data Organization</h4>

                                <p>The organization of data may vary from project to project. Here I will describe what was used for the explosive sync study as of August 2022.
                                </p>

                                <p>All subejct data comes to us via the fMRI Lab on North Campus and ends up in a directory labeled 'raw'. 
                                    In each subject's directory, there are several files and directories. For our purposes here, we are only interested in the 'DTI' and 'anatomy' directories. Within the 'DTI' directory, we will find several files that we need for our analyses. We will talk specifically about those files in subsequent sections. The most important thing for now is that you have DTI data. <br>Within the anatomy directory, you will probably see several directories. We are interested in the 't1spgr_208sl' directory, which is where our subject's T1 data is stored. Within the 't1spgr_208sl' directory, locate the <strong>t1spgr_208sl.nii</strong> file. If you don't see it, look around in other directories and then contact Scott Peltier (spelt@med.umich.edu) from the fMRI Lab if necessary.
                                </p>

                                <p>Once you've verified that you have both DTI and anatomical data, go back to the main study directory. An example of this would be something like: /PROJECTS/REHARRIS/explosives Once there, create a directory called 'dtiProc'. This will be where all the processed DTI data will be stored. Within dtiProc, create a subs directory and a scripts directory. Within the subs directory, each subject's dti, antomical and fieldmap data will be stored. Don't worry about creating those directories manually, they will be done automatically later. The majority of the commands that we run on the server will be executed on data stored in dtiProc as to not corrupt the raw data. Within the scripts directory, feel free to copy the scripts from this documentation, make any necessary edits, and save them in your scripts directory.
                                </p>
                            

                    </section>
                    <!-- end section -->

                    <section id="line2" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Creating Fieldmaps<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->


                        <p>Because of a recent update to the scanner software from GE, the first thing we have to do is create fieldmaps for each of our subjects. To do so, we will be using various fsl commands. In addition to creating fieldmaps for each subject, we also have to copy over the appropriate bval and bvec files. The bval and bvec files found in the 'DTI' folder are incorrect, so we will be using bval and bvec files originally from the ABCD study that have been modified for our needs. As a general rule, the number of lines in your bval and bvec files should match the number of volumes in your diffusion data. If you'd like to see an example of bval and bvec files, download <a href="abcd/abcd_example.zip" download="abcd_example">here</a>. The data from the explosive sync study has 104 volumes, so our bval and bvec files have been edited to match.
                        </p>

                        <p>Let's look at the first script below. This script will copy over the bvec and bval files that we've edited to the main 'DTI' directory and then create fieldmaps using a bash script (more on that later). You will need to edit lines 14 and 15 for your specific needs. The dtiProc directory should point to the new directory you created in the Data Organization section. The rawDir should point to the directory that contains all of your subjects' raw data. Line 16 can be left alone if you're working in the CPFRC environment, otherwise you want to change it to the name of the directory where each subject's diffusion lives. For example, any given subject in the rawDir may have several sub-directories including functional, anatomical and diffusion data. If the name of the diffusion data sub-directory is the same for each subject, change the value in line 16 accordingly.
                        </p>

                        <div class="row">
                            <div class="col-md-12">

                        <p id="line2_1">Now we will go function by function and discuss what is happening. You can download the script <a href="scripts/createFieldMap.py.zip" download="createFieldmap">here</a> if interested. The verifyModules function will check that the necessary software/modules for this script are loaded in the environment. In this case, it's just fsl. The bcolors class below it contains color variables that can be accessed as part of the verifyModules function. The getSubList function will create a list of subjects based on the rawDir variable. A subject will only be added to the list if they have a 'dti.nii' file in their 'DTI' directory. changeBVFiles will copy the edited bval and bvec files mentioned above to each subject's 'DTI' directory. You will need to edit lines 73 and 74 to point to the path where your bval and bvec files are stored. The createFieldmaps function is the core of the script. First, it will check if a fieldmap has already been created for the subject. If so, it will skip the processing step. If the subject doesn't have a fieldmap, the fslFMAP.sh script (found below) gets copied from the dtiProc/scripts directory to the individual subject directory. Once copied, it will be executed. After createFieldmaps processes each subject, the checkOutput function will go through each subject and check if the fieldmap was successfully created. You will receive a list of all (if any) subjects that did not run correctly.
                        </p>


                                <h4>Create Field Map Script</h4>
                            </div>
                        </div>

                                <pre class="brush: python">
                        ##### This script has functions that will:
# 1) generate a list of all the subjects that have been processed on new scanner software
# 2) copy over abcd bvec and bval files 
# 3) create fieldmaps using external fsl script
# 4) verify that the fieldmaps have been successfully created

import sys
import os
import shutil
import time


dtiProc = "/PROJECTS/REHARRIS/explosives/dtiProc"
rawDir = "/PROJECTS/REHARRIS/explosives/raw/"
niiPath = "/DTI"

# verify that all the necessary modules have been loaded

def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input(f"\n{bcolors.WARNING}For this script, you need the fsl module loaded.\nIs it listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n{bcolors.ENDC}")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load fsl using the following command:\n")
                print("module load fsl/6.0.3\n")
                print("after loading the module, relaunch the script")
                sys.exit()

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

# get a list of all the subjects that have been processed with the new software

def getSubList(rawDir):
    os.chdir(rawDir)
    subList = []
    for sub in os.listdir():
        os.chdir(sub)
        if os.path.isdir("DTI"):
            os.chdir("DTI")
            if os.path.isfile("dti.nii"):
                subList.append(sub)
        os.chdir(rawDir)
    return subList


## copies the abcd bvec and bval files

def changeBVFiles(rawDir, subList):
    os.chdir(rawDir)
    for sub in subList:
        os.chdir(sub + niiPath)
        if os.path.isfile("abcd_edit.bval") and os.path.isfile("abcd_edit.bvec"):
            print("abcd bval and bvec files have already been copied over for subject " + sub)
            time.sleep(.1)
            os.chdir(rawDir)
            continue
        else:
            print("copying abcd bval and bvec files for subject " + sub)
            shutil.copy("/home/dasay/diffusion/abcd_edit.bval", os.getcwd())
            shutil.copy("/home/dasay/diffusion/abcd_edit.bvec", os.getcwd())
        os.chdir(rawDir)

# creates fieldmaps for each subject


def createFieldmaps(rawDir, subList):
    os.chdir(rawDir)
    for sub in subList:
        os.chdir(sub + niiPath)
        if os.path.isfile("final_fieldmap.nii.gz"):
            print("fieldmap already exists for subject " + sub + "\nmoving to next subject...")
            time.sleep(.1)
            os.chdir(rawDir)
        else:
            print("creating fieldmap for subject " + sub + " ...")
            shutil.copy("/PROJECTS/REHARRIS/explosives/dtiProc/scripts/fslFMAP.sh", os.getcwd())
            os.system("bash fslFMAP.sh")
            os.chdir(rawDir)



def checkOutput(rawDir, subList):
    os.chdir(rawDir)
    badSubs = []
    for sub in subList:
        os.chdir(sub + niiPath)
        if os.path.isfile("final_fieldmap.nii.gz"):
            print("Fieldmap successfully created for subject " + sub)
            time.sleep(.1)
            os.chdir(rawDir)
        else:
            badSubs.append(sub)
            os.chdir(rawDir)
    print("No fieldmaps were created for the following subjects:\n" + str(badSubs) + "\nIf there are no subjects listed, then you're good!")

verifyModules()

subjects = getSubList(rawDir)

changeBVFiles(rawDir, subjects)

createFieldmaps(rawDir, subjects)

checkOutput(rawDir, subjects)

                                </pre>


                        <div class="row">
                            <div class="col-md-12">

                        <p id ="line2_2">The <a href = "scripts/fslFMAP.sh.zip" download="fslFMAP">bash script</a> below, named fslFMAP.sh, is being called by the python script above and will need to be saved somewhere on your machine before running the python script. Remember to edit the python script above on line 90 with the location of the script. You can probably tell that the python script largely serves as a wrapper for this bash script. Let's walk through the script and see what it's doing.
                        </p>

                        <p>First, we use <i>fslsplit</i> to create a .nii file for each of the volumes in the dti.nii data. Next, we use <i>fslmerge</i> to combine the reverse polarity .nii file and the first volume from the dti data. Make sure the fm_rev_pol.nii file is in each subject's 'DTI' directory; the script will fail without it. The fslsplit commmand generates a lot of unnecessary files, so we can clean that up using <i>rm dti_split*</i>. The following <i>fslroi</i> and <i>fslmerge</i> commands are padding the data for the topup command. For whatever reason, topup does not work with there are an odd number of slices, so we add one in here. With the data padded we're ready to run <i>topup</i>. When topup is done we will remove the padded slice with <i>fslsplit</i> and rename the output file to final_fieldmap.nii.gz. If all goes well, you should have a file named <strong>final_fieldmap.nii.gz</strong> for each of your subejcts.
                        </p>

                              </div>
                        </div>

                        <h4>fslFMAP Script</h4>

                        <pre class="brush: bash">
#!/bin/bash


# split the data into its respective volumes
echo 'Splitting Volumes...'
fslsplit dti.nii dti_split -t

# merge the rev pol data with slice 0
echo 'merging volumes...'
fslmerge -t FM_vols.nii fm_rev_pol.nii dti_split0000.nii

# clean up directory
rm dti_split*

# extract one slice
echo 'extracting slice...'
fslroi FM_vols.nii FM_oneslice.nii 0 -1 0 -1 0 1 0 -1 

# merge it to the data
echo 'merging into padded data...'
fslmerge -z FM_padded FM_oneslice FM_vols.nii

# run topup on FM_padded
echo 'running topup...'
topup --imain=FM_padded.nii --datain=datain.txt --config=b02b0.cnf --out=my_topup_results --iout=fieldmap_b0 --fout=calculated_fm

# get rid of last volume in fieldmap
echo 'removing last volume'
fslsplit fieldmap_b0.nii final_fieldmap -t
mv final_fieldmap0000.nii.gz final_fieldmap.nii.gz 
rm final_fieldmap0001.nii.gz

                        </pre>

                    </section>
                    <!-- end section -->

                    <section id="line3" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Copy and Convert the Data<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <p>With our fieldmaps created we are ready to move forward with data processing. First, we will copy the data from the rawDir to the each subject's directory under dtiProc. MRtrix uses a special file extension called .mif, so we will have to convert our nii data to mif format after it has been copied. Download the script <a href="scripts/preproc01.py.zip" download="copyConvert">here</a>. The downloadable script contains code for both this section and the DWI Denoise section.
                        </p>

                        <p>The script starts off with the verifyModules function which ensures that both Mrtrix and AFNI have both been loaded into the environment. The getSubList function will create a list of subjects from the rawSubDir. A subject gets added to the list if they have both dti data and have had a fieldmap created for them. Next, makeSubDirs will take the list created from getSubList and create a directory for each subject with dti, fieldmaps and anatomy sub-directories.
                        </p>
                        <p>After the directories are created, the copyData function will copy over data for the 3 modalities. copyData makes use of the checkIfDataCopied helper method to check if the data for each modality has already been copied over. This is where the resample function comes in. This function will ensure that the fieldmap and dti data have the same dimensions. If they don't, it will cause problems in later processing steps. The output of this function will be 'new_fieldmap.nii'. Once the data finishes copying and has been resampled, the renameAndConvert and convert functions work in tandem to convert the dti and fieldmap data from nii to mif format. Notice that the <i>mrconvert</i> command on lines 133-136 add the bval and bvec files to the header of the output file with the -fslgrad option. Your final outputs should be <strong>run-01_dwi.mif</strong> and <strong>fieldmap.mif</strong>.
                        </p>

                        <pre class="brush: python">

                        ### This scripts has functions that will: 
# 1) copy nifti data from raw dir to dtiProc, including fieldmaps, dti and anatomy
# 2) label dti and fieldmap data as PA and AP, respectively
# 3) convert the data from nii to mif format and combine with its bvec and bval files
# 4) compare the number of volumes in mif file and bvec/bval files, all 3 should be equal

import shutil
import os
import subprocess
import pandas as pd
import nibabel as nib
import numpy as np
import time

rawSubDir = "/PROJECTS/REHARRIS/explosives/raw/"
subsDir = "/PROJECTS/REHARRIS/explosives/dtiProc/subs/"
rawPath = "/DTI"


def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input(f"\n{bcolors.WARNING}For this script, you need the mrtrix module loaded.\nIs it listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n{bcolors.ENDC}")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load fsl using the following command:\n")
                print("module load mrtrix\n")
                print("after loading the module, relaunch the script")
                sys.exit()
 
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


# this function will create a list of subjects to process if their data was collected using the new scanner software AND createFieldMap.py has successfully run

def getSubList(rawSubDir):
    os.chdir(rawSubDir)
    subList = []
    for sub in os.listdir():
        os.chdir(sub)
        if os.path.isdir("DTI"):
            os.chdir("DTI")
            if os.path.isfile("dti.nii") and os.path.isfile("final_fieldmap.nii.gz"):
                subList.append(sub)
        os.chdir(rawSubDir)
    return subList


# this function will create a subject directory and dti, fieldmaps and anatomy sub-directories in the dtiProc/subs dir

def makeSubDirs(subDir, rawSubDir):
    for sub in getSubList(rawSubDir):
        os.chdir(subDir)
        if os.path.isdir(sub):
            continue
        else:
            os.makedirs(sub)
            os.chdir(sub)
            os.makedirs("dti")
            os.makedirs("fieldmaps")
            os.makedirs("anatomy")

# if subject data has not already been copied, this function will copy it over from raw to dtiProc

def copyData(subDir, rawSubDir, rawPath):
    print("Copying over raw data...")
    for sub in getSubList(rawSubDir):
        dataPath = rawSubDir + sub + rawPath
        newDtiPath = subDir + sub + "/dti"
        dataCopied = checkIfDataCopied(newDtiPath, sub, "dti")
        if dataCopied is False:
            dtiCopyList = ["/abcd_edit.bval", "/abcd_edit.bvec", "/dti.nii"]
            for file in dtiCopyList:
                source = dataPath + file
                destination = newDtiPath
                print("Copying " + file + " for subject " + sub)
                shutil.copy(source, destination)
        fieldmaps = rawSubDir + sub + rawPath
        newFieldmapPath = subDir + sub + "/fieldmaps"
        dataCopied = checkIfDataCopied(newFieldmapPath, sub, "fieldmap")
        if dataCopied is False:
            fmapCopyList = ["/final_fieldmap.nii.gz"]
            for file in fmapCopyList:
                source = fieldmaps + file
                destination = newFieldmapPath
                print("Copying " + file + " for subject " + sub)
                shutil.copy(source, destination)
        anatomical = rawSubDir + sub + "/anatomy/t1spgr_208sl"
        newAnatomyPath = subDir + sub + "/anatomy"
        dataCopied = checkIfDataCopied(newAnatomyPath, sub, "anatomical")
        if dataCopied is False:
            anatCopyList = ["/t1spgr_208sl.nii"]
            for file in anatCopyList:
                source = anatomical + file 
                destination = newAnatomyPath
                print("Copying " + file + " for subject " + sub)
                shutil.copy(source, destination)

# this function will check if the raw data for a given subject has already been copied from raw to dtiProc

def checkIfDataCopied(dataPath, sub, modality):
    os.chdir(dataPath)
    dirContents = os.listdir()
    if dirContents:
        print(str(modality) + " data already copied for subject: " + sub)
        return True
    else:
        return False

def resample(rawSubDir, subDir):
    for sub in getSubList(rawSubDir):
        print("Resampling the data for subject " + sub)
        dtiNiiPath = subDir + sub + "/dti/dti.nii"
        fieldmapPath = subDir + sub + "/fieldmaps"
        os.chdir(fieldmapPath)
        shutil.copy(dtiNiiPath, os.getcwd())
        resampleCommand = "3dresample -master dti.nii -prefix new_fieldmap.nii -input final_fieldmap.nii.gz"
        proc = subprocess.Popen(resampleCommand, shell=True, stdout=subprocess.PIPE)
        proc.wait()
        os.remove("dti.nii")
        time.sleep(.1)


# function will convert dti and fieldmap data from .nii to .mif format.

def convert(fieldmap, sub):
    if fieldmap is False:
        if os.path.isfile("already_converted.txt"):
            print("Dti files have already been combined and converted to .mif format for subject " + sub)
            return
        print("Converting dti files for subject: " + sub)
        convertToMif = f"""
            mrconvert \
            dti.nii \
            run-01_dwi.mif \
            -fslgrad abcd_edit.bvec abcd_edit.bval
        """
        proc1 = subprocess.Popen(convertToMif, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        subprocess.run(['touch', 'already_converted.txt'])
    else:
        if os.path.isfile("already_converted.txt"):
            print("Fmap files have already been combined and converted to .mif format for subject " + sub)
            return
        print("Converting fmap files for subject: " + sub)
        convertToMif = f"""
            mrconvert \
            new_fieldmap.nii \
            fieldmap.mif 
        """
        proc2 = subprocess.Popen(convertToMif, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        subprocess.run(['touch', 'already_converted.txt'])


# go into each subject directory and call convert() function

def renameAndConvert(subDir):
    print("Converting files from nii to mif format...")
    for sub in getSubList(rawSubDir):
        # Go into dti directory for the sub and convert data to .mif
        os.chdir(subDir)
        dtiData = sub + "/dti"
        os.chdir(dtiData)
        convert(False, sub) # False indicates not fieldmap data
        os.chdir(subDir)
        fmapData = sub + "/fieldmaps"
        os.chdir(fmapData)
        convert(True, sub) # True indicates fieldmap data

def runAll(subDir, rawSubDir, rawPath):
    verifyModules()
    makeSubDirs(subDir, rawSubDir)
    copyData(subDir, rawSubDir, rawPath)
    resample(rawSubDir, subDir)
    renameAndConvert(subDir)

runAll(subDir, rawSubDir, rawPath)


                        </pre>


                    </section>
                    <!-- end section -->

                    <section id="line4" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">DWI Denoise <hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">

                            <div class="col-md-12">
                                <p>With the data converted to mif format, we're ready to run the first pre-processing step: <i>dwidenoise</i>. The dwidenoise command creates a noise map estimation by running a principle components analysis (PCA). Read <a href="https://mrtrix.readthedocs.io/en/latest/reference/commands/dwidenoise.html" target="_blank">here</a> for a more in-depth explanation. 
                                </p>
                                <p>You will notice that the script below has the same preamble as the script in the Copy and Convert the Data section. You can combine these scripts into one or keep them separate. They've been split up here for readability. verifyModules and getSubList run first to make sure we've got our necessary software loaded and all the correct subjects queued for processing. Our dwiDenoise function will run the MRtrix <i>dwidenoise</i> on each subject while also utilizing the checkNoiseFile function to check if dwiDenoise has already been run on the subject. If it has, that subject will be skipped to avoid redundant processing. 
                                </p>
                                <p>Next, we get to the createB0 function. This function does several things for us. First, it will go into each subject directory in dtiProc and create a directory called 'combined'. The combined directory will serve as the host directory for several different data files that come from both the dti and fieldmap directories. The overall purpose of the createB0 function is to extract the b-values from both the reverse and primary phase encoded images and combine them into one file. You'll notice that I have the b-values extraction step commented out for the fieldmap (reverse phase encoded) data. In our case, since the fieldmap is just one volume, it is not necessary (or feasible) to do so. However, I've left the commands there as a reference for those who may be in a different situation. We extract the b-values using MRtrix's <i>dwiextract</i> and then comnbine the two files into a B0 image using MRtrix's <i>mrcat</i>. Our final output file should be <strong>b0_pair.mif</strong>. The fixZDir function serves a similar purpose to <i>fslmerge</i> a couple of sections ago. We will be running topup later on as a part of a preprocessing step, so we need to make sure the dimensions in the z direction are even. Here, we accomplish that by running <i>mrgrid</i> on our denoised data. Our output file will have the same name, run-01_den.mif, but you should see our z direction data go up by one when you run <i>mrinfo</i>.
                                </p>
                                <p>The final function for this section is fixDataStrides. This function may not be strictly necessary to run, but I've found it useful in the past. There may be instances where, somewhere during processing, the data strides information in the header of your diffusion data gets incorrectly edited. This function should fix that problem. I also use it as a chance to copy my bval and bvec files into the 'combined' directory. The fixDataStrides function simply uses MRtrix's <i>mrconvert</i> command to convert the data from mif to nii format, and then back to mif with the fslgrad option. You should only have to do this if the data strides in your data read as anything besides: [-1 2 3 4]. You can get this info by running <i>mrinfo</i> on your run-01_den.mif data. The runAll function will run all of the aforementioned functions.
                                </p>
                            </div>


                        <pre class="brush: python">

import sys
import os
import shutil
import time


dtiProc = "/PROJECTS/REHARRIS/explosives/dtiProc"
rawDir = "/PROJECTS/REHARRIS/explosives/raw/"
niiPath = "/DTI"

# verify that all the necessary modules have been loaded

def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input(f"\n{bcolors.WARNING}For this script, you need the fsl module loaded.\nIs it listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n{bcolors.ENDC}")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load fsl using the following command:\n")
                print("module load fsl/6.0.3\n")
                print("after loading the module, relaunch the script")
                sys.exit()

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

# get a list of all the subjects that have been processed with the new software

def getSubList(rawDir):
    os.chdir(rawDir)
    subList = []
    for sub in os.listdir():
        os.chdir(sub)
        if os.path.isdir("DTI"):
            os.chdir("DTI")
            if os.path.isfile("dti.nii"):
                subList.append(sub)
        os.chdir(rawDir)
    return subList


# run mrtrix function dwidenoise

def dwiDenoise(subDir):
    print("Running dwi_denoise on subjects...")
    for sub in getSubList(rawSubDir):
        dtiSubjectDir = subDir + sub + "/dti"
        os.chdir(dtiSubjectDir)
        if not checkNoiseFile(sub):
            print("Running on " + sub)
            denoise = "dwidenoise run-01_dwi.mif run-01_den.mif -noise noise.mif"
            proc = subprocess.Popen(denoise, shell=True, stdout=subprocess.PIPE)
            proc.wait()
        else:
            continue


def checkNoiseFile(sub):
    if os.path.isfile("run-01_den.mif"):
        print("dwi_denoise has already been run on subject: " + sub)
        return True
    else:
        return False

# this function will create a B0 image that combines the dti and fieldmap data 

def createB0(subDir):
    #Get b0 image
    for sub in getSubList(rawSubDir):
        currentSub = subDir + sub
        os.chdir(currentSub)
        if not os.path.isdir("combined"):
            os.makedirs("combined")
            combinedDir = os.getcwd() + "/combined"
            os.chdir("fieldmaps")
            shutil.copy("fieldmap.mif", combinedDir)
        print("Creating B0 image for subject: " + sub)
        # get mean for fmaps
        #os.chdir("fieldmaps")
        #fmapB0 = "mrconvert fieldmap.mif - | mrmath - mean meanReversed.mif -axis 3"
        #proc1 = subprocess.Popen(fmapB0, shell=True, stdout=subprocess.PIPE)
        #proc1.wait()
        #shutil.copy("meanReversed.mif", combinedDir)
        os.chdir(currentSub)
        # get mean for dti
        os.chdir("dti")
        dtiB0 = "dwiextract run-01_den.mif - -bzero | mrmath - mean meanPrimary.mif -axis 3"
        proc2 = subprocess.Popen(dtiB0, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        shutil.copy("meanPrimary.mif", combinedDir)
        shutil.copy("run-01_den.mif", combinedDir)
        # combine means into b0 image
        os.chdir(combinedDir)
        meanB0 = "mrcat fieldmap.mif meanPrimary.mif -axis 3 b0_pair.mif"
        proc3 = subprocess.Popen(meanB0, shell=True, stdout=subprocess.PIPE)
        proc3.wait()

#this function pads the data in the z direction so that there's not an uneven number for topup later on 
# during dwifslpreproc

def fixZDir(subDir):
    for sub in getSubList(rawSubDir):
        currentSub = subDir + sub + "/combined"
        os.chdir(currentSub)
        print("running mrgrid on subject " + sub)
        mrgrid = "mrgrid run-01_den.mif pad -axis 2 0,1 out.mif"
        proc1 = subprocess.Popen(mrgrid, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        os.remove("run-01_den.mif")
        rename = "mrconvert out.mif run-01_den.mif"
        proc2 = subprocess.Popen(rename, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        os.remove("out.mif")

def fixDataStrides(subDir):
    """
    This is to be run when the data strides read as anything else besides [-1 2 3 4]
    Use mrinfo to view data strides. This is a fix if dwifslpreproc does not run.
    """
    for sub in getSubList(rawSubDir):
        currentSub = subDir + sub + "/combined"
        os.chdir(currentSub)
        shutil.copy("/home/dasay/diffusion/abcd_edit.bval", os.getcwd())
        shutil.copy("/home/dasay/diffusion/abcd_edit.bvec", os.getcwd())
        mifToNii = "mrconvert run-01_den.mif tmp.nii"
        proc1 = subprocess.Popen(mifToNii, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        os.remove("run-01_den.mif")
        niiToMif = "mrconvert tmp.nii run-01_den.mif -fslgrad abcd_edit.bvec abcd_edit.bval"
        proc2 = subprocess.Popen(niiToMif, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        os.remove("tmp.nii")

def runAll(subDir, rawSubDir, rawPath):
    verifyModules()
    makeSubDirs(subDir, rawSubDir)
    copyData(subDir, rawSubDir, rawPath)
    dwiDenoise(subDir)
    createB0(subDir)
    fixDataStrides(subDir)


                        </pre>


                        </div>
                        <!-- end row -->

                    </section>
                    <!-- end section -->

                    <section id="line5" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Working on Armis<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <div class="row">

                            <div class="col-md-12">
                                <p>None of the commands we've run so far have demanded very much computational power. However, several of the commands/steps from this point on will run much faster on a supercomputer or HPC cluster than on a personal or local machine. In the case of the CPFRC, we use a cluster called Armis, provided by the University of Michigan for HIPPA protected data. Luckily, Armis exists in a SLURM environment, so the majority of the content found in the following scripts can be used in other SLURM environments found outside of the University of Michigan.
                                </p>
                                <p>Connecting to Armis- First, you'll have to connect to the lab server (make sure you're connected to the <a href="https://hits.medicine.umich.edu/accounts-access/wifi-networks/vpn-remote-network-access" target="_blank">VPN</a>). You can do this via the command line, NoMachine or Windows Remote Desktop. Once logged in, you can connect to Armis via the command line or <a href="https://arc.umich.edu/open-ondemand/" target="_blank">Open OnDemand</a>. I will only describe the command line interface here. Execute <i>ssh armis2.arc-ts.umich.edu</i> from the command line and you'll be prompted to enter a password. This will be your Level One password. Next, you'll be prompted to select one of three 2-factor authentication options via Duo. I prefer to use the first option, which sends a notification to your phone for you to approve, but you can use whichever method you like. You should see a 'success' message if everything goes well. The <i>ssh</i> command above can be a bit tedious to type, so I would suggest creating an alias for it and adding it to your .bash_profile or .bashrc file. Read about creating aliases <a href="https://linuxize.com/post/how-to-create-bash-aliases/" target="_blank">here</a>.
                                </p>
                                <p>Let's head over to where the explosive sync data is being stored here on Armis. Cd into the explosive sync directory: <i>cd /scratch/seharte_root/seharte99/shared_data/expl</i>.You should see several different directories. Notice the scripts directory with sbatch and pipeline sub-directories. This is where all the scripts that we'll run on Armis are stored, including cluster submissions and head node scripts. The sbatch sub-directory has a sub-directory for each of the jobs we'll submit to Armis. I've found that organizing the scripts in this mannner proves helpful as job scripts start to pile up. That being said, feel free to organize your scripts however makes most sense to you. 
                                </p>
                                <p>
                                Cd to the dtiProc directory. The dtiProc directory on Armis will largely mirror the dtiProc/subs directory on the lab server. To avoid entering your Level One password dozens of times, I would suggest using the linux commmand <i>rsync</i> and copy the entire dtiProc/subs directory to your Armis dtiProc directory. First, cd into your dtiProc directory on Armis. You could use a command that looks something like this to pull the data:<br><strong>rsync -aruv user@aneshartelab-prod01.med.umich.edu:/PROJECTS/REHARRIS/explosives/dtiProc/subs/ .</strong> This make take a while to run, depending on the number of files you have and their size. Once your data has finished transferring, you're ready to move on to the next preprocessing step: dwifslpreproc. See you there!
                                </p>
                            </div>

                    </section>
                    <!-- end section -->

                    <section id="line6" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">dwifslpreproc<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <p>MRtrix's <i>dwifslpreproc</i> command is the finale of all the preprocessing we've done up to this point. This command uses a blend of MRtrix original data preprocessing techniques along with FSL's topup and eddy current correction. To speed up processing time, we'll submit a processing job to Armis for each of the subjects. Without greater computing power and parallelization, it would take a regular computer upwards of 65 hours to process 30 subjects. An HPC cluster cuts that down to less than an hour. Download both scripts from this section <a href="scripts/dwifslpreproc.zip" download="dwifslpreproc">here</a>.
                        </p>

                        <p id="line6_1">Lines 1-20 comprise what's known as the sbatch preamble. This is how you request resources from the cluster management system and tell them which account to bill. Enter your email address on line 4 to be notified when the jobs complete, fail or are cancelled. Make sure you enter the faculty member's account you're using on line 7. Next, the script will load the necessary modules. Luckily for us, the <i>module load mrtrix</i> command also loads FSL and CUDA, which are necessary for dwifslpreproc to run in an HPC environment. You can edit the USERNAME on line 30 to whatever you'd like, but it is not strictly necessary. You will need to edit the SOURCE_DIR variable on line 36 to contain the path to your project's "home" directory. Line 40 copies the data from the combined directory to the temporary BIDS directory. Lines 42-55 simply print out some information about the job being executed on Armis. Lines 65-69 contain the actual dwifslpreproc command that we're running. Once it's finished running, the output will be copied to a specified directory on line 75. If all goes well, you should have output data for all subjects at <i>/scratch/seharte_root/seharte99/shared_data/expl/dwifslpreproc</i>
                        </p>

                        <h4>Preproc Sbatch Script</h4>

                        <pre class="brush: bash">
#!/bin/bash

###SBATCH --job-name=$subject
#SBATCH --mail-user=youremail@example.com
#SBATCH --mail-type=END,FAIL

#SBATCH --account=FACULTY ACCOUNT
#SBATCH --partition=gpu

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

#SBATCH --gpus-per-task=4
#SBATCH --mem-per-gpu=8gb

#SBATCH --time=1:00:00
#SBATCH --export=ALL

#### End SBATCH preamble

module purge
module load mrtrix

my_job_header

# Set participant name
participant=$subject

# Create a local directory in which to work
TMPDIR=$(mktemp -d /tmp/USERNAME-dtiMrtrix3.XXXXXXXXX)
cd $TMPDIR
mkdir BIDS

# Set the names of the dwifslpreproc diretories. 

SOURCE_DIR=/scratch/seharte_root/seharte99/shared_data/expl
BIDS_DIR=$PWD/BIDS

# Get the needed BIDS data; print only the summary statistics from the copy
rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/explBIDS/$participant/combined/ ./BIDS

# Print some information about the run that might be useful
echo "#---------------------------------------------------------------------#"
echo "Running on           :  $(hostname -s)"
echo "Processor type       :  $(lscpu | grep 'Model name' | sed 's/[ \t][ ]*/ /g')"
echo "Assigned processors  :  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.cpus)"
echo "Assigned memory nodes:  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.mems)"
echo "======================================================================="
echo "/tmp space"
df -h /tmp
echo "======================================================================="
echo "Memory usage"
free
echo "#---------------------------------------------------------------------#"
echo


# Run it
source /etc/profile.d/http_proxy.sh

cd BIDS

#### dwipreproc command

dwifslpreproc run-01_den.mif run-01_den_preproc.mif \
    -nocleanup \
    -pe_dir PA \
    -rpe_pair -se_epi b0_pair.mif \
    -eddy_options " --slm=linear --data_is_shelled"


# Copy the results out of TMPDIR
echo "Copying $OUTPUT_DIR/$participant to ${SOURCE_DIR}/dwifslpreproc"
mkdir -p ${SOURCE_DIR}/dwifslpreproc/${participant}
rsync -arv ${BIDS_DIR}/ ${SOURCE_DIR}/dwifslpreproc/${participant}

# Change out of the $TMPDIR and remove it
cd
rm -rf $TMPDIR


                        </pre>

                    <br>

                    <p id="line6_2">Running the script above requires the use of what's commonly called a job submit script. It's pretty simple to use, but there are a few key details that should not be overlooked. Take a look at the example below. We have a for-loop that's looping through a text file containing a list of all the subjects we want to run. Make sure that text file is in the same directory as your job script. You don't need to edit lines six or seven. On line eight, you need to tell the script where to spit out the log file. You also need to make sure the directory you point it to is created <i>before</i> you execute the job script. Finally, make sure you include the name of your sbatch script at the end of line eight. To execute this script, type <i>source submitdwifslpreproc.sh</i>. You should see several messages in the terminal indicating that your jobs were submitted to the cluster. You can check the status of your jobs by typing <i>squeue -u your_username</i> in the command window.
                    </p>

                    <h4>Preproc Job Script</h4>

                    <pre class="brush: bash">
#!/bin/bash

for subject in $(cat sublist.txt); do

        export subject
        echo "Submitting $subject"
        sbatch --job-name=$subject --output=/scratch/seharte_root/seharte99/shared_data/expl/job_output/$subject-%j.log dwifslpreprocSbatch.sh

done
                    </pre>

                    </section>
                    <!-- end section -->

                    <section id="line7" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Spherical Deconvolution<hr></h2>


                            <p>Congratulations, you're done with preprocessing! Now we'll start getting into the core of our diffusion pipeline. We'll discuss Constrained Spherical Deconvolution (CSD) in this section. The general idea behind CSD is to create a basis function from each subject's data to determine the orientation of diffusion in each voxel. This estimation is largely based on the type of tissue found in a given voxel and is similar to the hemodynamic response function used in fMRI analyses. However, before getting into our CSD analysis, we will create a brain mask to speed up our processing time. The script below details the mask generation process along with a CSD analysis. Download it <a href="scripts/preproc02.py.zip" download="sphericalDeconvolution">here</a>. This script is pretty hefty, but it's not as bad as it might look. Almost half of the functions serve as helpers to other functions or to avoid extra, unnecessary processing. Let's dive in.
                            </p>

                            <p>A nice thing about this script is that it really only needs to be edited in one place: line seven. Update procDir to be equal to the path to your dtiProc directory. Now we'll go function by function and discuss a little bit of what's happening. The <i>checkIfCompleted</i> function will iterate through each subject's directory in the dtiProc directory and check if the final output file, <strong>wmfod_norm.mif</strong>, is present. If so, the script has already been run on that subject and will not be run again. The subject gets added to the noRunSubList array which gets passed to all subsequent functions. We've seen <i>verifyModules</i> in previous sections. We need MRtrix and ANTs loaded for this script to work. 
                            </p>

                            <p><i>dwibiascorrect</i> kicks off the mask creation portion of this script. The <i>dwibiascorrect</i> command from MRtrix removes inhomogeneities from the data in preparation to create our brain mask. <i>getSubListBiasCorrect</i> collects a list of subjects that have not already had dwibiascorrect run on them. If the command has already been executed and the output file exists, there's no reason to run it again. You'll notice that every MRtrix/ANTs command we run in this script has an accompanying getSubList function to make sure we're not re-doing any work. With inhomogeneities removed, we're ready to create the brain mask using <i>dwi2mask</i>. Creating a brain mask significantly reduces processing time by removing non-brain voxels from our analysis. The <i>getSubListDwiMask</i> function checks if a mask has already been created for our subjects.
                            </p>

                            <p>The <i>dwi2Response</i> function is the first step in our CSD analysis. This MRtrix command decomposes the diffusion signal into smaller fiber orientations, creating fiber orientation distributions (FODs) that are specific to each individual subject. The output for this function will be three .txt files, one for each tissue type. These files are estimates of the ammount of diffusion in each of the three orthogonal directions. The <i>getSubListDwiResponse</i> checks if each subject already has the output from dwi2response. Next, we come to the <i>dwi2Fod</i> function. This function takes the basis functions generated by the dwi2response command and applies them to the diffusion data. The output will be .mif images for each tissue type: white matter, grey matter and cerebrospinal fluid. Within <i>dwi2Fod</i> we'll also run <i>mrconvert</i> to combine all three tissue types into one image file. <i>getSubListDwiFod</i> checks if dwi2Fod has already been run.
                            <br>Finally, we arrive at <i>normalizeData</i>. This function is here just in case you decide to run group level analyses on your data. If not, you can omit this function. Similarly to its predecessors, <i>getSubListNormalize</i> checks if <i>normalizeData</i> has already been run on each subject. On to the next step!
                            </p>

                            <h4>Constrained Spherical Deconvolution Script</h4>


                            <pre class="brush: python">
import os
import subprocess
import sys


procDir = "/scratch/seharte_root/seharte99/shared_data/expl/dtiProc"


# This function will check if the script has already been run to completion previously

def checkIfCompleted(procDir):
    os.chdir(procDir)
    noRunSubList = []
    for sub in os.listdir(os.getcwd()):
        os.chdir(sub)
        if os.path.isfile("wmfod_norm.mif"):
            print("all steps have already been run on subject " + sub)
            noRunSubList.append(sub)
            os.chdir(procDir)
        os.chdir(procDir)
    return noRunSubList

# This function will check if the necessary modules have been loaded

def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input("\nFor this script, you need the mrtrix and ANTs modules loaded.\nAre they listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load mrtrix and ANTs using the following command:\n")
                print("module load mrtrix ANTs\n")
                print("after loading the modules, relaunch the script")
                sys.exit()

# runs dwibiascorrect on all subjects fed in with list

def dwibiascorrect(procDir, noRunList):
    for sub in getSubListBiasCorrect(procDir, noRunList):
        os.chdir(sub)
        print("running dwibiascorrect on subject " + sub + "...")
        dwibiascorrect = "dwibiascorrect ants run-01_den_preproc.mif run-01_den_preproc_unbiased.mif -bias bias.mif"
        proc1 = subprocess.Popen(dwibiascorrect, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("run-01_den_preproc_unbiased.mif"):
            print("dwibiascorrect ran successfully for subject " + sub)
        else:
            print("dwibiascorrect did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

# checks if dwibiascorrect has already been run on the given subjects

def getSubListBiasCorrect(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("run-01_den_preproc_unbiased.mif"):
            print("dwibiascorrect has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

# runs dwi2mask on all given subjects

def dwi2mask(procDir, noRunList):
    for sub in getSubListDwiMask(procDir, noRunList):
        os.chdir(sub)
        print("running dwi2mask on subject " + sub + "...")
        dwi2mask = "dwi2mask run-01_den_preproc_unbiased.mif mask.mif"
        proc1 = subprocess.Popen(dwi2mask, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("mask.mif"):
            print("dwi2mask ran successfully for subject " + sub)
        else:
            print("dwi2mask did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

# checks if dwi2mask has already been run on the given subjects

def getSubListDwiMask(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("mask.mif"):
            print("dwi2mask has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

# runs dwi2response command with dhollander algorithm on all given subjects

def dwi2Response(procDir, noRunList):
    for sub in getSubListDwiResponse(procDir, noRunList):
        os.chdir(sub)
        print("running dwi2response on subject " + sub + "...")
        dwi2response = "dwi2response dhollander run-01_den_preproc_unbiased.mif wm.txt gm.txt csf.txt -voxels voxels.mif"
        proc1 = subprocess.Popen(dwi2response, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("wm.txt"):
            print("dwi2response ran successfully for subject " + sub)
        else:
            print("dwi2response did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

# checks if dwi2response has already been run on the given subjects

def getSubListDwiResponse(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("wm.txt"):
            print("dwi2response has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

# runs dwi2fod command on all available subjects

def dwi2Fod(procDir, noRunList):
    for sub in getSubListDwiFod(procDir, noRunList):
        os.chdir(sub)
        print("running dwi2fod on subject " + sub + "...")
        dwi2Fod = "dwi2fod msmt_csd run-01_den_preproc_unbiased.mif -mask mask.mif wm.txt wmfod.mif gm.txt gmfod.mif csf.txt csffod.mif"
        proc1 = subprocess.Popen(dwi2Fod, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("wmfod.mif"):
            print("dwi2Fod ran successfully for subject " + sub)
        else:
            print("dwi2Fod did NOT run successfully for subject " + sub + ". please check")
        print("combining fod data...")
        combineFod = "mrconvert -coord 3 0 wmfod.mif - | mrcat csffod.mif gmfod.mif - vf.mif"
        proc2 = subprocess.Popen(combineFod, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        if os.path.isfile("vf.mif"):
            print("sucessfully combined data.")
        else:
            print("data did not successfully combine for subject " + sub + ". please check")
        os.chdir(procDir)

# checks if dwi2fod has already been run on the given subjects

def getSubListDwiFod(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("vf.mif"):
            print("dwi2Fod has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

# runs mtnormalise command on all available subjects

def normalizeData(procDir, noRunList):
    for sub in getSubListNormalize(procDir, noRunList):
        os.chdir(sub)
        print("running normalization on subject " + sub + "...")
        normalize = "mtnormalise wmfod.mif wmfod_norm.mif gmfod.mif gmfod_norm.mif csffod.mif csffod_norm.mif -mask mask.mif"
        proc1 = subprocess.Popen(normalize, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("wmfod_norm.mif"):
            print("normalization ran successfully for subject " + sub)
        else:
            print("normalization did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

# checks if mtnormalise has already been run on the given subjects

def getSubListNormalize(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("wmfod_norm.mif"):
            print("normalization has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

# execute all the functions in this order

verifyModules()
checkIfCompleted(procDir)
noRunList = checkIfCompleted(procDir)
dwibiascorrect(procDir, noRunList)
dwi2mask(procDir, noRunList)
dwi2Response(procDir, noRunList)
dwi2Fod(procDir, noRunList)
normalizeData(procDir, noRunList)

                            </pre>

                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                    </section>
                    <!-- end section -->

                    <section id="line8" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Create Tissue Boundaries<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                            <p>Part of our ultimate goal in this pipeline is to run a streamline analysis which will place seeds at random locations along the grey-white matter boundary. A streamline grows from each seed until it terminates in another region. A necessary precursor to the streamline analysis is creating said boundary between the grey and white matter. As in previous sections, we'll walk through this script function by function. Downloaded it <a href="scripts/preproc03.py.zip" download="tissueBoundary">here</a>.
                            </p>

                            <p>Our script starts with the procDir variable that needs to be updated to reflect the path to the dtiProc directory for your project. The first two functions are probably starting to look familiar at this point, our old friends <i>verifyModules</i> and <i>checkIfCompleted</i>. The <i>checkIfCompleted</i> function looks for the final output file, <strong>gmwmSeed_coreg.mif</strong>, in each subject's directory. If it's there, that subject will be skipped. Note that individual subject files will have to be deleted if re-processing is desired. For this script <i>verifyModules</i> will just make sure that we have MRtrix and FSL loaded up.
                            </p>

                            <p>This script marks the beginning of our use of the anatomical data. The first thing we have to do is get the anatomical data into mif format. <i>segmentAnat</i> does that for us using MRtrix's <i>mrconvert</i> command. After the conversion is done, <i>segmentAnat</i> will run MRtrix's <i>5ttgen</i> command with the fsl option. <i>5ttgen</i> segments the anatomical image into five tissue types: grey matter, subcortical grey matter, white matter, cerebrospinal fluid and pathological tissue. After <i>segmentAnat</i>, we get to <i>dwiExtract</i>. This function performs a coregistration of our anatomical and diffusion data. Once that's finished, we're ready to create our white/grey matter boundary!
                            </p>

                            <p>If you read ahead in the script, you probably noticed that our next function, <i>fslFlirt</i>, is a bit of a monster. This is largely due to the fact that this step is being carried out with FSL commands, so we have to convert the format of our data from mif to nii and back to mif. We'll convert both the segmented anatomical image and the coregistered image to nii.gz format. After they're converted, we need to change the coregistered image from a 4D dataset to a single 3D image for the FSL flirt command using FSL's <i>fslroi</i> command. Moving on to FSL's <i>flirt</i> command, this command will coregister the combined diffusion/anatomical data with the grey matter segmentation data. The output of <i>flirt</i> is a matrix which overlays the diffusion image on the grey matter segmentation. With this matrix in hand, we can convert it to a format that can be read by MRtrix using the <i>transformconvert</i> and <i>mrtransform</i> commands. Finally, we arrive at the grande finale of this script: creating the boundary. We use MRtrix's <i>5tt2gmwi</i> command and voilà! We have our grey-white matter boundary. On to recon-all!
                            </p>


                    <h4>Tissue Boundaries Script</h4>

                    <pre class="brush: python">
import os
import subprocess
import sys


procDir = "/scratch/seharte_root/seharte99/shared_data/expl/dtiProc"


def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input("\nFor this script, you need the mrtrix and fsl modules loaded.\nAre they listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load mrtrix and fsl using the following command:\n")
                print("module load mrtrix fsl\n")
                print("after loading the modules, relaunch the script")
                sys.exit()


def checkIfCompleted(procDir):
    os.chdir(procDir)
    noRunSubList = []
    for sub in os.listdir(os.getcwd()):
        os.chdir(sub)
        if os.path.isfile("gmwmSeed_coreg.mif"):
            print("all steps have already been run on subject " + sub)
            noRunSubList.append(sub)
            os.chdir(procDir)
        os.chdir(procDir)
    return noRunSubList


def segmentAnat(prodDir, noRunList):
    for sub in getSubListSegmentAnat(procDir, noRunList):
        os.chdir(sub)
        print("running mrconvert on subject " + sub + "...")
        mrconvert = "mrconvert t1spgr_208sl.nii T1.mif"
        proc1 = subprocess.Popen(mrconvert, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("T1.mif"):
            print("mrconvert ran successfully for subject " + sub)
        else:
            print("mrconvert did NOT run successfully for subject " + sub + ". please check")
        print("running 5ttgen on subject " + sub + "...")
        segment = "5ttgen fsl T1.mif 5tt_nocoreg.mif"
        proc2 = subprocess.Popen(segment, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        if os.path.isfile("5tt_nocoreg.mif"):
            print("5ttgen ran successfully for subject " + sub)
        else:
            print("5ttgen did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

def getSubListSegmentAnat(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("5tt_nocoreg.mif"):
            print("tissue segmentation has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

def dwiExtract(procDir, noRunList):
    for sub in getSubListDwiExtract(procDir, noRunList):
        os.chdir(sub)
        print("running dwiextract on subject " + sub + "...")
        dwiextract = "dwiextract run-01_den_preproc_unbiased.mif - -bzero | mrmath - mean mean_b0.mif -axis 3"
        proc1 = subprocess.Popen(dwiextract, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("mean_b0.mif"):
            print("dwiextract ran successfully for subject " + sub)
        else:
            print("dwiextract did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

def getSubListDwiExtract(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("mean_b0.mif"):
            print("dwiextract has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

def fslFlirt(procDir, noRunList):
    for sub in getSubListFslFlirt(procDir, noRunList):
        os.chdir(sub)

        # run 2 mrconvert commands for work in fsl
        print("running mrconvert steps on subject " + sub + "...")
        convertB0ToNii = "mrconvert mean_b0.mif mean_b0.nii.gz"
        proc1 = subprocess.Popen(convertB0ToNii, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        convertNocoregToNii = "mrconvert 5tt_nocoreg.mif 5tt_nocoreg.nii.gz"
        proc2 = subprocess.Popen(convertNocoregToNii, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        if os.path.isfile("mean_b0.nii.gz") and os.path.isfile("5tt_nocoreg.nii.gz"):
            print("mrconvert ran successfully for subject " + sub)
        else:
            print("mrconvert did NOT run successfully for subject " + sub + ". please check")

        # run fslroi to make a 3D rather than 4D dataset
        print("running fslroi on subject " + sub + "...")
        fslroi = "fslroi 5tt_nocoreg.nii.gz 5tt_vol0.nii.gz 0 1"
        proc3 = subprocess.Popen(fslroi, shell=True, stdout=subprocess.PIPE)
        proc3.wait()
        if os.path.isfile("5tt_vol0.nii.gz"):
            print("fslroi ran successfully for subject " + sub)
        else:
            print("fslroi did NOT run successfully for subject " + sub + ". please check")

        # run flirt on subject
        print("running fsl flirt on subject " + sub + "...")
        flirt = "flirt -in mean_b0.nii.gz -ref 5tt_vol0.nii.gz -interp nearestneighbour -dof 6 -omat diff2struct_fsl.mat"
        proc4 = subprocess.Popen(flirt, shell=True, stdout=subprocess.PIPE)
        proc4.wait()
        if os.path.isfile("diff2struct_fsl.mat"):
            print("flirt ran successfully for subject " + sub)
        else:
            print("flirt did NOT run successfully for subject " + sub + ". please check")

        # run 2 transformation commands
        print("running transformation commands on subject " + sub + "...")
        transformconvert = "transformconvert diff2struct_fsl.mat mean_b0.nii.gz 5tt_nocoreg.nii.gz flirt_import diff2struct_mrtrix.txt"
        proc5 = subprocess.Popen(transformconvert, shell=True, stdout=subprocess.PIPE)
        proc5.wait()
        mrtransform = "mrtransform 5tt_nocoreg.mif -linear diff2struct_mrtrix.txt -inverse 5tt_coreg.mif"
        proc6 = subprocess.Popen(mrtransform, shell=True, stdout=subprocess.PIPE)
        proc6.wait()
        if os.path.isfile("5tt_coreg.mif"):
            print("transformations ran successfully for subject " + sub)
        else:
            print("transformations did NOT run successfully for subject " + sub + ". please check")

        os.chdir(procDir)

def getSubListFslFlirt(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("5tt_coreg.mif"):
            print("flirt steps has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

def createBoundary(procDir, noRunList):
    for sub in getSubListCreateBoundary(procDir, noRunList):
        os.chdir(sub)
        print("running 5tt2gmwmi on subject " + sub + "...")
        boundary = "5tt2gmwmi 5tt_coreg.mif gmwmSeed_coreg.mif"
        proc1 = subprocess.Popen(boundary, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile("gmwmSeed_coreg.mif"):
            print("5tt2gmwmi ran successfully for subject " + sub)
        else:
            print("5tt2gmwmi did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

def getSubListCreateBoundary(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile("gmwmSeed_coreg.mif"):
            print("5tt2gmwmi has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList



checkIfCompleted(procDir)
verifyModules()
noRunList = checkIfCompleted(procDir)
segmentAnat(procDir, noRunList)
dwiExtract(procDir, noRunList)
fslFlirt(procDir, noRunList)
createBoundary(procDir, noRunList)
                    </pre>

                    </section>
                    <!-- end section -->

                    <section id="line9" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">FreeSurfer Recon-all<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <p>Pause for a second and congratulate yourself for making it to this point in the documentation. We've covered a lot of ground; hopefully you've found success processing your data thus far. In this section, we're taking a small detour out of MRtrix to FreeSurfer. FreeSurfer is a neuroimaging software that specializes in analysis of anatomical images. Our ultimate goal of creating a connectome is made possible by FreeSurfer's <i>recon-all</i> command. <i>recon-all</i> performs several functions. Most notably for our purposes, it takes a 3D anatomical volume and reconstructs it into a two-dimensional surface and performs atlas-based parcellation. <i>recon-all</i> is very computationally intensive so we will be submiting another job to Armis to help speed up the process. Download the sbatch and job scripts <a href="scripts/recon-all.zip" download="recon_all">here</a>.
                        </p>

                        <p id="line9_1">Our sbatch script here is similar to the one we created for <i>dwifslpreproc</i>, but with a few key differences. Like before, you will need to make edits on lines five and eight in the preamble. Notice that the partition on line nine says 'standard' this time instead of 'gpu' and we've increased the memory and cpu allocation on lines 14 and 15. You can optionally make an edit on line 32 to the name of the TMPDIR. On line 45, notice that we are only copying the subject's T1 data into the BIDS directory. Though this should've been taken care of automatically in previous steps, make sure that each subject's anatomical data is in their dtiProc directory. You'll have to edit line 45 to reflect the path to your data along with the name of your T1 file. You don't need to mess with lines 47-60 unless you want to. On line 71, make sure your <i>recon-all</i> command accurately reflects the name of your T1 input file. After <i>recon-all</i> finishes running, it will export the results to whatever path you specify as seen on line 78. The max wall-time is set for 12 hours in this script, but it should take somewhere between 8-10 hours for most subjects. <i>recon-all</i> spits out tons of files, so don't get overwhelmed when you see the output!
                        </p>

                        <h4>Recon-all Sbatch Script</h4>

                        <pre class="brush: bash">
#!/bin/bash

###SBATCH --job-name=$subject
#SBATCH --mail-user=user@example.com
#SBATCH --mail-type=END,FAIL

#SBATCH --account=FACULTY ACCOUNT
#SBATCH --partition=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

#SBATCH --cpus-per-task=8
#SBATCH --mem=16gb

#SBATCH --time=12:00:00
#SBATCH --export=ALL

#### End SBATCH preamble

module purge
module load fsl
module load freesurfer

my_job_header

# Set participant name
participant=$subject

# Create a local directory in which to work

TMPDIR=$(mktemp -d /tmp/USERNAME-recon-all.XXXXXXXXX)
cd $TMPDIR
mkdir BIDS


# Set the names of the FreeSurfer diretories. 

SOURCE_DIR=/scratch/seharte_root/seharte99/shared_data/expl
BIDS_DIR=$PWD/BIDS
FREESURFER_WORK=$BIDS_DIR
SUBJECTS_DIR=$BIDS_DIR

# Get the needed BIDS data; print only the summary statistics from the copy
rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/dtiProc/$participant/t1spgr_208sl.nii ./BIDS

# Print some information about the run that might be useful
echo "#---------------------------------------------------------------------#"
echo "Running on           :  $(hostname -s)"
echo "Processor type       :  $(lscpu | grep 'Model name' | sed 's/[ \t][ ]*/ /g')"
echo "Assigned processors  :  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.cpus)"
echo "Assigned memory nodes:  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.mems)"
echo "======================================================================="
echo "/tmp space"
df -h /tmp
echo "======================================================================="
echo "Memory usage"
free
echo "#---------------------------------------------------------------------#"
echo



# Run it
source /etc/profile.d/http_proxy.sh

cd BIDS

### recon-all command

recon-all -i t1spgr_208sl.nii \
    -s sub-${participant}_T1w_recon -all

# Copy the results out of TMPDIR
echo "Copying $OUTPUT_DIR/$participant to ${SOURCE_DIR}/recon-all"
mkdir -p ${SOURCE_DIR}/recon-all/${participant}
rsync -arv ${BIDS_DIR}/ ${SOURCE_DIR}/recon-all/${participant}/

# Change out of the $TMPDIR and remove it
cd
rm -rf $TMPDIR

                        </pre>

                    <br>

                    <p id="line9_2">This job script functions very similarly to our <i>dwifslpreproc</i> job script. The only differences are the path to the log file on line eight and the name of the sbatch script that's being called (recon_allSbatch.sh). Edit the line eight according to your needs and make sure the sublist.txt file and sbatch script are in the same directory. Execute the job script by typing <i>source recon_allJob.sh</i> in the command window. You'll see several 'job submitted' messages in the command window. Use <i>squeue -u username</i> to check the status of your jobs. You'll also get emailed when each job is completed or fails.
                    </p>

                    <h4>Recon-all Job Script</h4>

                    <pre class="brush: bash">
#!/bin/bash

for subject in $(cat sublist.txt); do

        export subject
        echo "Submitting $subject"
        sbatch --job-name=$subject --output=/scratch/seharte_root/seharte99/shared_data/expl/job_output/recon-all/$subject-%j_recon_all.log recon_allSbatch.sh

done

                    </pre>

                    </section>
                    <!-- end section -->

                    <section id="line10" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Generating Streamlines<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->

                        <p>The streamlines we're creating in this section represent the white matter tracts that connect anatomically distinct regions of grey matter. These estimates that MRtrix creates are probabilistically created based on anatomically constrained tractography (ACT). ACT will only label a streamline as valid if it is biologically plausible. For example, an invalid streamline could be one that terminates in CSF and thus is not biologically plausible. We can create the streamlines with MRtrix's <i>tckgen</i> command. We will be creating 10 million streamlines so it will take some time. Sounds like a perfect job for Armis! We will also include <i>tcksift2</i> in our sbatch script. <i>tcksift2</i> serves to correct any overfitting of streamlines that may have happened during the <i>tckgen</i> command. Overfitting can happen when certain tracts have clearer FOD's, thus making them better candidates for the probabalistic modeling that ACT does. <i>tcksift2</i> helps us hedge against that possibility. Download the scripts <a href="scripts/tractography.zip" download="generate_streamlines">here</a>.
                        </p>

                        <p id="line10_1">Just as in previous sbatch scripts we've created, you will need to edit the email and faculty account in your own script along with the name of the TMPDIR on line 31. You'll notice that lines 40-42 have several <i>rsync</i> commands. We just need three files from each subject's dtiProc directory, so there's no reason to copy the entire directory into our temporary processing directory. The <i>rsync</i> command on line 44 can be uncommented if you've already run <i>tckgen</i> on a subject and you just want to run <i>tcksift2</i>. Lines 69-72 and 76-78 contain our <i>tckgen</i> and <i>tcksift2</i> commands. The output files from our commands will be sent to a directory named 'tractography'. Feel free to tweak the output data's destination. Now that we've got our streamlines, we're ready to put it all together and create our connectome!
                        </p>

                        <h4>Streamlines Sbatch Script</h4>

                    <pre class="brush: bash">
#!/bin/bash

###SBATCH --job-name=$subject
#SBATCH --mail-user=user@example.com
#SBATCH --mail-type=END,FAIL

#SBATCH --account=FACULTY ACCOUNT
#SBATCH --partition=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

#SBATCH --cpus-per-task=8
#SBATCH --mem=16gb

#SBATCH --time=5:00:00
#SBATCH --export=ALL

#### End SBATCH preamble

module purge
module load mrtrix

my_job_header

# Set participant name
participant=$subject

# Create a local directory in which to work
TMPDIR=$(mktemp -d /tmp/USERNAME-tractography.XXXXXXXXX)
cd $TMPDIR
mkdir BIDS

# Set the names of the tractography diretories.
SOURCE_DIR=/scratch/seharte_root/seharte99/shared_data/expl
BIDS_DIR=$PWD/BIDS

# Get the needed BIDS data; print only the summary statistics from the copy
rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/dtiProc/$participant/5tt_coreg.mif ./BIDS
rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/dtiProc/$participant/gmwmSeed_coreg.mif ./BIDS
rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/dtiProc/$participant/wmfod_norm.mif ./BIDS
# only do this if first command was run previously
#rsync -a --info=STATS /scratch/seharte_root/seharte99/shared_data/expl/dtiProc/$participant/tracks_10M.tck ./BIDS

# Print some information about the run that might be useful
echo "#---------------------------------------------------------------------#"
echo "Running on           :  $(hostname -s)"
echo "Processor type       :  $(lscpu | grep 'Model name' | sed 's/[ \t][ ]*/ /g')"
echo "Assigned processors  :  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.cpus)"
echo "Assigned memory nodes:  $(cat /sys/fs/cgroup/cpuset/slurm/uid_${EUID}/job_${SLURM_JOBID}/cpuset.mems)"
echo "======================================================================="
echo "/tmp space"
df -h /tmp
echo "======================================================================="
echo "Memory usage"
free
echo "#---------------------------------------------------------------------#"
echo


# Run it
source /etc/profile.d/http_proxy.sh

cd BIDS

### tractography command

tckgen -act 5tt_coreg.mif \
    -backtrack -seed_gmwmi gmwmSeed_coreg.mif \
    -nthreads 8 -maxlength 250 -cutoff 0.06 \
    -select 10000000 wmfod_norm.mif tracks_10M.tck

### overfitting command

tcksift2 -act 5tt_coreg.mif \
    -out_mu sift_mu.txt -out_coeffs sift_coeffs.txt \
    -nthreads 8 tracks_10M.tck wmfod_norm.mif sift_1M.txt

# Copy the results out for posterity
echo "Copying $OUTPUT_DIR/$participant to ${SOURCE_DIR}/tractography"
mkdir -p ${SOURCE_DIR}/tractography/${participant}
rsync -arv ${BIDS_DIR}/ ${SOURCE_DIR}/tractography/${participant}

# Change out of the $TMPDIR and remove it
cd
rm -rf $TMPDIR

                    </pre>

                    <br>

                    <p id="line10_2">Our job script here is the same as our previous iterations. Update the log file path and the name of the sbatch file to match your own. Ensure that your sublist.txt file and sbatch script are in the same directory as this job script. Execute this script then sit back and "watch" your streamlines get created! 
                    </p>

                    <h4>Streamlines Job Script</h4>

                    <pre class="brush: bash">
#!/bin/bash

for subject in $(cat sublist.txt); do

        export subject
        echo "Submitting $subject"
        sbatch --job-name=$subject --output=/scratch/seharte_root/seharte99/shared_data/expl/job_output/tractography/$subject-%j_tracto.log tractographySbatch.sh

done

                    </pre>


                    <section id="line11" class="section">

                        <div class="row">
                            <div class="col-md-12 left-align">
                                <h2 class="dark-text">Creating the Connectome<hr></h2>
                            </div>
                            <!-- end col -->
                        </div>
                        <!-- end row -->
                        <p>You've made it to the climax of this documentation: creating the connectome. Let's finish out strong.<br>Our connectome script starts out similarly to our previous Python scripts; we have a couple of path variables to update and we have the familiar <i>verifyModules</i> and <i>checkIfCompleted</i> functions. Our procDir path variable is the same as previous scripts. The recon-allDir needs to point to the output of your recon-all command. This script requires MRtrix and FreeSurfer and our final output file is a csv file called <strong>$SUBNAME_parcels.csv</strong>. You may have noticed that the <i>verifyModules</i> script now has a yellow color warning message. That's where the <i>bcolors</i> class from line 113 is being used. I'm experimenting with the yellow warning message versus the default black and white and any feedback/expressed peference would be appreciated. Download the script <a href="scripts/connectome.py.zip" download="create_connectome">here</a>. 
                        </p>

                        <p>The first function we come across after <i>verifyModules</i> and <i>checkIfCompleted</i> is <i>labelConvert</i> on line 38. <i>labelConvert</i> will help us make use of the parcellations that FreeSurfer created for us. First, it will copy each subject's aparc+aseg.mgz file to their dtiProc directory. Next, it will convert the aparc file into a format that MRtrix can understand via the FreeSurfer's <i>labelconvert</i> command. Finally, the <i>tck2Connectome</i> functions takes us home. We use MRtrix's <i>tck2connectome</i> command to combine the streamlines and parcellations to create our connectome and spit out a csv file. 
                        </p>


                    <pre class="brush: python">
import os
import subprocess
import sys


procDir = "/scratch/seharte_root/seharte99/shared_data/expl/dtiProc"
recon_allDir = "/scratch/seharte_root/seharte99/shared_data/expl/recon-all"


def verifyModules():
    print("Modules loaded:")
    os.system("module list")
    while True:
            loaded = input(f"\n{bcolors.WARNING}For this script, you need the mrtrix and freesurfer modules loaded.\nAre they listed above? If so, type yes and hit enter. \nIf not, please type no and hit enter.\n{bcolors.ENDC}")
            if loaded == "yes" or loaded == "y" or loaded == "Yes" or loaded == "YES":
                print("great! running script...")
                break
            elif loaded == "no" or loaded == "n" or loaded == "No" or loaded == "NO":
                print("\nplease load mrtrix and freesurfer using the following command:\n")
                print("module load mrtrix freesurfer\n")
                print("after loading the module, relaunch the script")
                sys.exit()


def checkIfCompleted(procDir):
    os.chdir(procDir)
    noRunSubList = []
    for sub in os.listdir(os.getcwd()):
        os.chdir(sub)
        if os.path.isfile(f"{sub}_parcels.csv"):
            print("all steps have already been run on subject " + sub)
            noRunSubList.append(sub)
            os.chdir(procDir)
        os.chdir(procDir)
    return noRunSubList

def labelConvert(procDir, noRunList, recon_allDir):
    for sub in getSubListLabelConvert(procDir, noRunList):
        os.chdir(sub)
        print("copying aparc+aseg.mgz file to dtiProc...")
        copyAparc = f"cp {recon_allDir}/{sub}/mri/aparc+aseg.mgz ."
        proc1 = subprocess.Popen(copyAparc, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile(f"aparc+aseg.mgz"):
            print("successfully copied aparc+aseg.mgz for subject " + sub)
        else:
            print("did NOT successfully copy aparc+aseg.mgz for subject " + sub + ". please check")
        print("running labelconvert on subject " + sub + "...")
        labelconvert = f"""
                labelconvert aparc+aseg.mgz $FREESURFER_HOME/FreeSurferColorLUT.txt \
                /sw/arcts/centos7/mrtrix/3.0.3/share/mrtrix3/labelconvert/fs_default.txt \
                {sub}_parcels.mif
                """
        proc2 = subprocess.Popen(labelconvert, shell=True, stdout=subprocess.PIPE)
        proc2.wait()
        if os.path.isfile(f"{sub}_parcels.mif"):
            print("labelconvert ran successfully for subject " + sub)
        else:
            print("labelconvert did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

def getSubListLabelConvert(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile(f"{sub}_parcels.mif"):
            print("labelconvert has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

def tck2Connectome(procDir, noRunList):
    for sub in getSubListTck2Connectome(procDir, noRunList):
        os.chdir(sub)
        print("running tck2connectome on subject " + sub + "...")
        tck2connectome = f"""
                    tck2connectome -symmetric -zero_diagonal -scale_invnodevol \
                    -tck_weights_in sift_1M.txt tracks_10M.tck \
                    {sub}_parcels.mif {sub}_parcels.csv \
                    -out_assignment assignments_{sub}_parcels.csv
                    """
        proc1 = subprocess.Popen(tck2connectome, shell=True, stdout=subprocess.PIPE)
        proc1.wait()
        if os.path.isfile(f"{sub}_parcels.csv"):
            print("tck2connectome ran successfully for subject " + sub)
        else:
            print("tck2connectome did NOT run successfully for subject " + sub + ". please check")
        os.chdir(procDir)

def getSubListTck2Connectome(procDir, noRunList):
    os.chdir(procDir)
    subList = []
    for sub in os.listdir(os.getcwd()):
        if sub in noRunList:
            continue
        os.chdir(sub)
        if os.path.isfile(f"{sub}_parcels.csv"):
            print("tck2connectome has already been run on subject " + sub + ". Moving to next subject...")
            os.chdir(procDir)
            continue
        else:
            subList.append(sub)
        os.chdir(procDir)
    return subList

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


checkIfCompleted(procDir)
verifyModules()
noRunList = checkIfCompleted(procDir)
labelConvert(procDir, noRunList, recon_allDir)
tck2Connectome(procDir, noRunList)

                        
                    </pre>

                    <p>
                    </p>

                    </section>
                    <!-- end section -->
                </div>
                <!-- // end .col -->

            </div>
            <!-- // end .row -->

        </div>
        <!-- // end container -->

    </div>
    <!-- end wrapper -->

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/retina.js"></script>
    <script src="js/jquery.fitvids.js"></script>
    <script src="js/wow.js"></script>
    <script src="js/jquery.prettyPhoto.js"></script>

    <!-- CUSTOM PLUGINS -->
    <script src="js/custom.js"></script>
    <script src="js/main.js"></script>

    <script src="js/syntax-highlighter/scripts/shCore.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushXml.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushCss.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushJScript.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushPython.js"></script>
    <script src="js/syntax-highlighter/scripts/shBrushBash.js"></script>

</body>

</html>
